{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd84f04d-3c03-421a-ac26-29e13858743e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T22:09:14.422761Z",
     "iopub.status.busy": "2025-10-25T22:09:14.422459Z",
     "iopub.status.idle": "2025-10-25T22:09:14.542382Z",
     "shell.execute_reply": "2025-10-25T22:09:14.541601Z",
     "shell.execute_reply.started": "2025-10-25T22:09:14.422738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Write test successful\n",
      "✅ Read test successful: Hello, ML Pipeline!\n",
      "✅ Cleanup successful\n",
      "\n",
      "Your S3 access is working perfectly!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sanity check making sure I can write to the s3 bucket I created for this project.\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket = 'sagemaker-bucket-ml-data-pipeline' \n",
    "\n",
    "# Test write\n",
    "test_content = \"Hello, ML Pipeline!\"\n",
    "s3.put_object(Bucket=bucket, Key='test.txt', Body=test_content)\n",
    "print(\"✅ Write test successful\")\n",
    "\n",
    "# Test read\n",
    "response = s3.get_object(Bucket=bucket, Key='test.txt')\n",
    "content = response['Body'].read().decode('utf-8')\n",
    "print(f\"✅ Read test successful: {content}\")\n",
    "\n",
    "# Clean up test file\n",
    "s3.delete_object(Bucket=bucket, Key='test.txt')\n",
    "print(\"✅ Cleanup successful\")\n",
    "\n",
    "print(\"\\nYour S3 access is working perfectly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd2737a-626c-4399-b748-7cd811889db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T22:12:18.211787Z",
     "iopub.status.busy": "2025-10-25T22:12:18.211487Z",
     "iopub.status.idle": "2025-10-25T22:12:18.326478Z",
     "shell.execute_reply": "2025-10-25T22:12:18.325458Z",
     "shell.execute_reply.started": "2025-10-25T22:12:18.211765Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket = 'sagemaker-bucket-ml-data-pipeline'  # put your bucket here\n",
    "\n",
    "# Upload raw Titanic data, make sure the file is in the correct location\n",
    "s3.upload_file('train.csv', bucket, 'ml-pipeline/raw/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dacf1b-9c46-4a85-bafa-0db2eaf2c946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T23:01:32.264593Z",
     "iopub.status.busy": "2025-10-25T23:01:32.264325Z",
     "iopub.status.idle": "2025-10-25T23:01:32.270119Z",
     "shell.execute_reply": "2025-10-25T23:01:32.269200Z",
     "shell.execute_reply.started": "2025-10-25T23:01:32.264573Z"
    }
   },
   "source": [
    "## **Creating roles for AWS Glue**\n",
    "\n",
    "First, need to create the roles for it.\n",
    "\n",
    "1. **IAM Console → Roles → Create role**\n",
    "2. **Trusted entity type:** AWS service\n",
    "3. **Use case:** Glue (it auto-creates the trust policy!)\n",
    "4. **Permissions:** \n",
    "   - Search and add: `AWSGlueServiceRole` (managed policy)\n",
    "   - Click \"Next\"\n",
    "5. **Role name:** `GlueETLRole`\n",
    "6. **Create role**\n",
    "\n",
    "7. **Then add S3 permissions:**\n",
    "   - Find your new `GlueETLRole`\n",
    "   - Click \"Add permissions\" → \"Create inline policy\"\n",
    "   - Visual editor:\n",
    "     - Service: S3\n",
    "     - Actions: GetObject, PutObject, ListBucket, DeleteObject\n",
    "     - Resources: \n",
    "       - Bucket: `sagemaker-bucket-ml-data-pipeline`\n",
    "       - Object: `sagemaker-bucket-ml-data-pipeline/*` (This will auto-update to \"*\")\n",
    "   - Name: `S3Access`\n",
    "   - Click Create policy\n",
    "\n",
    "## **Creating the AWS Glue job**\n",
    "\n",
    "1. **AWS Console → Glue → ETL Jobs**\n",
    "2. **Click \"Create job\"**\n",
    "3. **Choose \"Spark script editor\"**\n",
    "4. **Paste your glue_etl.py**\n",
    "5. **Set job details:**\n",
    "\n",
    " - Name: titanic-etl-job\n",
    " - IAM Role: Create new or use existing (needs S3 + Glue permissions)\n",
    " - Glue version: 4.0 (latest)\n",
    " - Language: Python 3\n",
    "\n",
    "*Additional job params:*\n",
    " - Add parameter: --INPUT_PATH = s3://sagemaker-bucket-ml-data-pipeline/ml-pipeline/raw/titanic.csv\n",
    " - Add parameter: --OUTPUT_PATH = s3://sagemaker-bucket-ml-data-pipeline/ml-pipeline/processed/\n",
    "\n",
    "6. Save and Run\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b6bcb5-9d22-494e-8078-d1d005fd9611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T23:15:05.109640Z",
     "iopub.status.busy": "2025-10-25T23:15:05.108940Z",
     "iopub.status.idle": "2025-10-25T23:15:06.363282Z",
     "shell.execute_reply": "2025-10-25T23:15:06.361390Z",
     "shell.execute_reply.started": "2025-10-25T23:15:05.109608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files created by Glue:\n",
      "  ml-pipeline/processed/part-00000-3fef16c9-ca7d-4e6a-98ce-1266b2282c16-c000.snappy.parquet (0.01 MB)\n",
      "\n",
      "✅ Successfully read processed data!\n",
      "Shape: (891, 8)\n",
      "Columns: ['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'FamilySize', 'IsAlone']\n",
      "\n",
      "First few rows:\n",
      "   Survived  Pclass  Sex   Age     Fare  Embarked  FamilySize  IsAlone\n",
      "0         0       3    0  22.0   7.2500         2           1        0\n",
      "1         1       1    1  38.0  71.2833         0           1        0\n",
      "2         1       3    1  26.0   7.9250         2           0        1\n",
      "3         1       1    1  35.0  53.1000         2           1        0\n",
      "4         0       3    0  35.0   8.0500         2           0        1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" Verifying the data loaded correctly \"\"\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket = 'sagemaker-bucket-ml-data-pipeline'\n",
    "\n",
    "# List processed files\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix='ml-pipeline/processed/'\n",
    ")\n",
    "\n",
    "print(\"Processed files created by Glue:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    size_mb = obj['Size'] / (1024 * 1024)\n",
    "    print(f\"  {obj['Key']} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# Read one of the Parquet files to verify\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "\n",
    "# Get the parquet file (there might be multiple parts)\n",
    "parquet_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.parquet')]\n",
    "\n",
    "if parquet_files:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=parquet_files[0])\n",
    "    df = pd.read_parquet(io.BytesIO(obj['Body'].read()))\n",
    "    \n",
    "    print(f\"\\n✅ Successfully read processed data!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b3d8e0-8248-4338-993e-88f52dd2f82e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T23:25:26.561474Z",
     "iopub.status.busy": "2025-10-25T23:25:26.561149Z",
     "iopub.status.idle": "2025-10-25T23:28:13.924913Z",
     "shell.execute_reply": "2025-10-25T23:28:13.924049Z",
     "shell.execute_reply.started": "2025-10-25T23:25:26.561450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-25-23-25-26-666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data location: s3://sagemaker-bucket-ml-data-pipeline/ml-pipeline/processed/\n",
      "Starting training...\n",
      "2025-10-25 23:25:28 Starting - Starting the training job...\n",
      "2025-10-25 23:25:41 Starting - Preparing the instances for training...\n",
      "2025-10-25 23:26:29 Downloading - Downloading the training image......\n",
      "2025-10-25 23:27:19 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-10-25 23:27:27.163 ip-10-0-95-85.us-west-2.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-10-25 23:27:27.191 ip-10-0-95-85.us-west-2.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] Train matrix has 891 rows and 7 columns\u001b[0m\n",
      "\u001b[34m[2025-10-25 23:27:27.594 ip-10-0-95-85.us-west-2.compute.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-10-25 23:27:27.595 ip-10-0-95-85.us-west-2.compute.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-10-25 23:27:27.597 ip-10-0-95-85.us-west-2.compute.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-10-25 23:27:27.598 ip-10-0-95-85.us-west-2.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-10-25:23:27:27:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.89006\u001b[0m\n",
      "\u001b[34m[2025-10-25 23:27:27.610 ip-10-0-95-85.us-west-2.compute.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-10-25 23:27:27.613 ip-10-0-95-85.us-west-2.compute.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.89311\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.89565\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.89661\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.90799\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.91649\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.92124\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.92296\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.92617\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.92903\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.93116\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.93332\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.93370\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.93415\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.93683\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.93693\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.93761\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.93851\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.93888\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.93897\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.94059\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.94081\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.94202\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.94191\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.94288\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.94624\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.94838\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.94911\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.94991\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.95107\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.95135\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.95208\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.95348\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.95392\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.95449\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.95532\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.95572\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.95620\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.95661\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.95727\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.96051\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.96115\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.96114\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.96203\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.96238\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.96344\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.96467\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.96483\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.96544\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.96576\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.96614\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.96656\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.96714\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.96744\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.96759\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.96920\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.96931\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.97006\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.97056\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.97114\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.97162\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.97176\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.97242\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.97269\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.97321\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.97404\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.97437\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.97439\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.97502\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.97502\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.97544\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.97551\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.97575\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.97577\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.97611\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.97673\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.97707\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.97748\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.97782\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.97872\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.97884\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.97882\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.97892\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.97889\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.97901\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.97922\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.97925\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.97935\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.97943\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.97981\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.98009\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.98057\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.98066\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.98087\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.98101\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.98107\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.98121\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.98125\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.98177\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.98197\u001b[0m\n",
      "\n",
      "2025-10-25 23:27:48 Uploading - Uploading generated training model\n",
      "2025-10-25 23:27:48 Completed - Training job completed\n",
      "Training seconds: 104\n",
      "Billable seconds: 104\n",
      "\n",
      "✅ Training complete!\n",
      "Model location: s3://sagemaker-bucket-ml-data-pipeline/ml-pipeline/models/sagemaker-xgboost-2025-10-25-23-25-26-666/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = 'sagemaker-bucket-ml-data-pipeline'\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Point to the PROCESSED data from Glue\n",
    "train_path = f's3://{bucket}/ml-pipeline/processed/'\n",
    "print(f\"Training data location: {train_path}\")\n",
    "\n",
    "# Get XGBoost container\n",
    "container = image_uris.retrieve('xgboost', sess.boto_region_name, '1.5-1')\n",
    "\n",
    "# Create estimator\n",
    "xgb = Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{bucket}/ml-pipeline/models/',\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Train on processed Parquet data\n",
    "train_input = TrainingInput(\n",
    "    train_path, \n",
    "    content_type='application/x-parquet'  # Note: Parquet, not CSV!\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "xgb.fit({'train': train_input})\n",
    "\n",
    "print(f\"\\n✅ Training complete!\")\n",
    "print(f\"Model location: {xgb.model_data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
