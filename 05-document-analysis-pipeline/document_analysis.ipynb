{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5517b8bf-afb1-4952-bee3-3d8b3ea6a2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Reading from: s3://sagemaker-us-east-2-854757836160/document-analysis/uploads/sample_feedback.txt\n",
      "\n",
      "‚úÖ Extracted Text:\n",
      "\n",
      "Customer Feedback Form\n",
      "\n",
      "Name: John Smith\n",
      "Date: November 6, 2024\n",
      "Product: Widget Pro 2000\n",
      "\n",
      "Feedback:\n",
      "I absolutely love this product! The quality exceeded my expectations and \n",
      "the customer service was outstanding. I had a small issue with setup but \n",
      "the support team resolved it within 30 minutes. Highly recommend to anyone \n",
      "considering this purchase. Five stars!\n",
      "\n",
      "Would you recommend to others? Yes\n",
      "Overall satisfaction: 10/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Setup\n",
    "sess = sagemaker.Session()\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = sess.default_bucket()\n",
    "\n",
    "# Key is just the path within the bucket (no s3://, no bucket name)\n",
    "document_key = \"document-analysis/uploads/sample_feedback.txt\"\n",
    "\n",
    "print(f\"Reading from: s3://{bucket_name}/{document_key}\")\n",
    "\n",
    "# Read text file from S3\n",
    "response = s3.get_object(Bucket=bucket_name, Key=document_key)\n",
    "extracted_text = response['Body'].read().decode('utf-8')\n",
    "\n",
    "print(\"\\n‚úÖ Extracted Text:\")\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3433794d-4100-48b2-9711-93f290cad721",
   "metadata": {},
   "source": [
    "***Run Amazon Comprehend***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9371059a-d04d-4886-9f4d-6068a3d00272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing text with Amazon Comprehend...\n",
      "\n",
      "1Ô∏è‚É£ SENTIMENT ANALYSIS\n",
      "--------------------------------------------------\n",
      "Overall Sentiment: POSITIVE\n",
      "  Positive: 100.0%\n",
      "  Negative: 0.0%\n",
      "  Neutral:  0.0%\n",
      "  Mixed:    0.0%\n",
      "\n",
      "2Ô∏è‚É£ ENTITY RECOGNITION\n",
      "--------------------------------------------------\n",
      "Found 7 entities:\n",
      "\n",
      "  ‚Ä¢ John Smith\n",
      "    Type: PERSON\n",
      "    Confidence: 99.9%\n",
      "\n",
      "  ‚Ä¢ November 6, 2024\n",
      "    Type: DATE\n",
      "    Confidence: 100.0%\n",
      "\n",
      "  ‚Ä¢ Widget\n",
      "    Type: COMMERCIAL_ITEM\n",
      "    Confidence: 58.7%\n",
      "\n",
      "  ‚Ä¢ Pro 2000\n",
      "    Type: TITLE\n",
      "    Confidence: 52.6%\n",
      "\n",
      "  ‚Ä¢ 30 minutes\n",
      "    Type: QUANTITY\n",
      "    Confidence: 94.7%\n",
      "\n",
      "  ‚Ä¢ Five stars\n",
      "    Type: QUANTITY\n",
      "    Confidence: 99.7%\n",
      "\n",
      "  ‚Ä¢ 10/10\n",
      "    Type: QUANTITY\n",
      "    Confidence: 95.5%\n",
      "\n",
      "3Ô∏è‚É£ KEY PHRASES\n",
      "--------------------------------------------------\n",
      "Found 17 key phrases:\n",
      "\n",
      "  ‚Ä¢ Customer Feedback Form\n",
      "\n",
      "Name (95.0%)\n",
      "  ‚Ä¢ John Smith\n",
      "Date (96.5%)\n",
      "  ‚Ä¢ November 6, 2024 (99.9%)\n",
      "  ‚Ä¢ Product (96.1%)\n",
      "  ‚Ä¢ Widget Pro 2000\n",
      "\n",
      "Feedback (94.7%)\n",
      "  ‚Ä¢ this product (100.0%)\n",
      "  ‚Ä¢ The quality (99.9%)\n",
      "  ‚Ä¢ my expectations (100.0%)\n",
      "  ‚Ä¢ the customer service (100.0%)\n",
      "  ‚Ä¢ a small issue (100.0%)\n",
      "\n",
      "‚úÖ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "comprehend = boto3.client('comprehend')\n",
    "\n",
    "# Use the extracted_text from previous step\n",
    "print(\"üîç Analyzing text with Amazon Comprehend...\\n\")\n",
    "\n",
    "# 1. Sentiment Analysis\n",
    "print(\"1Ô∏è‚É£ SENTIMENT ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "sentiment_response = comprehend.detect_sentiment(\n",
    "    Text=extracted_text,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "sentiment = sentiment_response['Sentiment']\n",
    "scores = sentiment_response['SentimentScore']\n",
    "\n",
    "print(f\"Overall Sentiment: {sentiment}\")\n",
    "print(f\"  Positive: {scores['Positive']:.1%}\")\n",
    "print(f\"  Negative: {scores['Negative']:.1%}\")\n",
    "print(f\"  Neutral:  {scores['Neutral']:.1%}\")\n",
    "print(f\"  Mixed:    {scores['Mixed']:.1%}\")\n",
    "\n",
    "# 2. Entity Recognition\n",
    "print(\"\\n2Ô∏è‚É£ ENTITY RECOGNITION\")\n",
    "print(\"-\" * 50)\n",
    "entities_response = comprehend.detect_entities(\n",
    "    Text=extracted_text,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "entities = entities_response['Entities']\n",
    "print(f\"Found {len(entities)} entities:\\n\")\n",
    "\n",
    "for entity in entities:\n",
    "    print(f\"  ‚Ä¢ {entity['Text']}\")\n",
    "    print(f\"    Type: {entity['Type']}\")\n",
    "    print(f\"    Confidence: {entity['Score']:.1%}\\n\")\n",
    "\n",
    "# 3. Key Phrases\n",
    "print(\"3Ô∏è‚É£ KEY PHRASES\")\n",
    "print(\"-\" * 50)\n",
    "phrases_response = comprehend.detect_key_phrases(\n",
    "    Text=extracted_text,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "key_phrases = phrases_response['KeyPhrases']\n",
    "print(f\"Found {len(key_phrases)} key phrases:\\n\")\n",
    "\n",
    "for phrase in key_phrases[:10]:  # Show top 10\n",
    "    print(f\"  ‚Ä¢ {phrase['Text']} ({phrase['Score']:.1%})\")\n",
    "\n",
    "# Store results for next phase\n",
    "analysis_result = {\n",
    "    'document': document_key,\n",
    "    'sentiment': {\n",
    "        'overall': sentiment,\n",
    "        'scores': scores\n",
    "    },\n",
    "    'entities': [\n",
    "        {\n",
    "            'text': e['Text'],\n",
    "            'type': e['Type'],\n",
    "            'score': e['Score']\n",
    "        }\n",
    "        for e in entities\n",
    "    ],\n",
    "    'key_phrases': [\n",
    "        {\n",
    "            'text': p['Text'],\n",
    "            'score': p['Score']\n",
    "        }\n",
    "        for p in key_phrases\n",
    "    ],\n",
    "    'metadata': {\n",
    "        'text_length': len(extracted_text),\n",
    "        'word_count': len(extracted_text.split())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4fa4b-62c4-4f12-b60e-e15d9e0b4e73",
   "metadata": {},
   "source": [
    "**Store analysis to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75c539a-3adf-45ec-b7d9-40163104e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text saved: s3://sagemaker-us-east-2-854757836160/document-analysis/processed/text/sample_feedback.txt\n",
      "‚úÖ Analysis saved: s3://sagemaker-us-east-2-854757836160/document-analysis/processed/analysis/sample_feedback_analysis.json\n",
      "‚úÖ Summary saved: s3://sagemaker-us-east-2-854757836160/document-analysis/processed/analysis/sample_feedback_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "\n",
      "Document Analysis Report\n",
      "Generated: 2025-11-08 18:55:46\n",
      "==========================================================\n",
      "\n",
      "Document: document-analysis/uploads/sample_feedback.txt\n",
      "Word Count: 65\n",
      "\n",
      "SENTIMENT ANALYSIS\n",
      "------------------\n",
      "Overall Sentiment: POSITIVE\n",
      "Confidence Scores:\n",
      "  Positive: 100.0%\n",
      "  Negative: 0.0%\n",
      "  Neutral:  0.0%\n",
      "  Mixed:    0.0%\n",
      "\n",
      "ENTITIES DETECTED\n",
      "-----------------\n",
      "\n",
      "PERSON:\n",
      "  - John Smith\n",
      "\n",
      "DATE:\n",
      "  - November 6, 2024\n",
      "\n",
      "COMMERCIAL_ITEM:\n",
      "  - Widget\n",
      "\n",
      "TITLE:\n",
      "  - Pro 2000\n",
      "\n",
      "QUANTITY:\n",
      "  - Five stars\n",
      "  - 30 minutes\n",
      "  - 10/10\n",
      "\n",
      "KEY PHRASES\n",
      "-----------\n",
      "  - Customer Feedback Form\n",
      "\n",
      "Name\n",
      "  - John Smith\n",
      "Date\n",
      "  - November 6, 2024\n",
      "  - Product\n",
      "  - Widget Pro 2000\n",
      "\n",
      "Feedback\n",
      "  - this product\n",
      "  - The quality\n",
      "  - my expectations\n",
      "  - the customer service\n",
      "  - a small issue\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Save extracted text\n",
    "text_key = document_key.replace('uploads/', 'processed/text/')\n",
    "s3.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=text_key,\n",
    "    Body=extracted_text.encode('utf-8')\n",
    ")\n",
    "print(f\"‚úÖ Text saved: s3://{bucket_name}/{text_key}\")\n",
    "\n",
    "# Save analysis JSON\n",
    "analysis_key = document_key.replace('uploads/', 'processed/analysis/').replace('.txt', '_analysis.json')\n",
    "s3.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=analysis_key,\n",
    "    Body=json.dumps(analysis_result, indent=2).encode('utf-8'),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "print(f\"‚úÖ Analysis saved: s3://{bucket_name}/{analysis_key}\")\n",
    "\n",
    "# Create and save summary report\n",
    "summary = f\"\"\"\n",
    "Document Analysis Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "==========================================================\n",
    "\n",
    "Document: {document_key}\n",
    "Word Count: {analysis_result['metadata']['word_count']}\n",
    "\n",
    "SENTIMENT ANALYSIS\n",
    "------------------\n",
    "Overall Sentiment: {analysis_result['sentiment']['overall']}\n",
    "Confidence Scores:\n",
    "  Positive: {analysis_result['sentiment']['scores']['Positive']:.1%}\n",
    "  Negative: {analysis_result['sentiment']['scores']['Negative']:.1%}\n",
    "  Neutral:  {analysis_result['sentiment']['scores']['Neutral']:.1%}\n",
    "  Mixed:    {analysis_result['sentiment']['scores']['Mixed']:.1%}\n",
    "\n",
    "ENTITIES DETECTED\n",
    "-----------------\n",
    "\"\"\"\n",
    "\n",
    "# Group entities by type\n",
    "entity_types = {}\n",
    "for entity in analysis_result['entities']:\n",
    "    etype = entity['type']\n",
    "    if etype not in entity_types:\n",
    "        entity_types[etype] = []\n",
    "    entity_types[etype].append(entity['text'])\n",
    "\n",
    "for etype, items in entity_types.items():\n",
    "    summary += f\"\\n{etype}:\\n\"\n",
    "    for item in set(items):  # Remove duplicates\n",
    "        summary += f\"  - {item}\\n\"\n",
    "\n",
    "summary += \"\\nKEY PHRASES\\n-----------\\n\"\n",
    "for phrase in analysis_result['key_phrases'][:10]:\n",
    "    summary += f\"  - {phrase['text']}\\n\"\n",
    "\n",
    "# Save summary\n",
    "summary_key = analysis_key.replace('.json', '_summary.txt')\n",
    "s3.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=summary_key,\n",
    "    Body=summary.encode('utf-8')\n",
    ")\n",
    "print(f\"‚úÖ Summary saved: s3://{bucket_name}/{summary_key}\")\n",
    "\n",
    "# Display the summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(summary)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7206059e-fe87-4278-8d35-c31f3df13080",
   "metadata": {},
   "source": [
    "**COMPLETE PIPELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c9bd3da-7d67-4d88-9461-a33c259d59d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing complete pipeline...\n",
      "\n",
      "üöÄ Starting pipeline for: document-analysis/uploads/sample_feedback.txt\n",
      "============================================================\n",
      "\n",
      "üìÑ Step 1: Extracting text...\n",
      "   ‚úì Extracted 428 characters from text file\n",
      "\n",
      "üîç Step 2: Analyzing with Comprehend...\n",
      "   ‚úì Sentiment: POSITIVE\n",
      "   ‚úì Found 7 entities\n",
      "   ‚úì Found 17 key phrases\n",
      "\n",
      "üìä Step 3: Compiling results...\n",
      "\n",
      "üíæ Step 4: Saving results to S3...\n",
      "   ‚úì Text: document-analysis/processed/text/sample_feedback.txt\n",
      "   ‚úì Analysis: document-analysis/processed/analysis/sample_feedback_analysis.json\n",
      "   ‚úì Summary: document-analysis/processed/analysis/sample_feedback_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline complete!\n",
      "============================================================\n",
      "\n",
      "üìã Quick Summary:\n",
      "  Sentiment: POSITIVE\n",
      "  Entities: 7\n",
      "  Key Phrases: 17\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def process_document_pipeline(bucket, document_key):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Extract text ‚Üí Analyze ‚Üí Store results\n",
    "    \n",
    "    Args:\n",
    "        bucket: S3 bucket name\n",
    "        document_key: Path to document in S3 (e.g., 'document-analysis/uploads/file.txt')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting pipeline for: {document_key}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    textract = boto3.client('textract')\n",
    "    comprehend = boto3.client('comprehend')\n",
    "    \n",
    "    # Step 1: Extract text\n",
    "    print(\"\\nüìÑ Step 1: Extracting text...\")\n",
    "    \n",
    "    try:\n",
    "        if document_key.endswith('.txt'):\n",
    "            # Simple text file - read directly\n",
    "            response = s3.get_object(Bucket=bucket, Key=document_key)\n",
    "            extracted_text = response['Body'].read().decode('utf-8')\n",
    "            print(f\"   ‚úì Extracted {len(extracted_text)} characters from text file\")\n",
    "        else:\n",
    "            # PDF or image - use Textract\n",
    "            print(f\"   Using Textract for: {document_key.split('.')[-1].upper()}\")\n",
    "            response = textract.detect_document_text(\n",
    "                Document={'S3Object': {'Bucket': bucket, 'Name': document_key}}\n",
    "            )\n",
    "            extracted_text = \"\"\n",
    "            for item in response['Blocks']:\n",
    "                if item['BlockType'] == 'LINE':\n",
    "                    extracted_text += item['Text'] + '\\n'\n",
    "            print(f\"   ‚úì Extracted {len(extracted_text)} characters with Textract\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error extracting text: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Analyze with Comprehend\n",
    "    print(\"\\nüîç Step 2: Analyzing with Comprehend...\")\n",
    "    \n",
    "    try:\n",
    "        # Limit text to 5000 characters (Comprehend limit)\n",
    "        text_to_analyze = extracted_text[:5000]\n",
    "        \n",
    "        # Sentiment\n",
    "        sentiment_response = comprehend.detect_sentiment(\n",
    "            Text=text_to_analyze,\n",
    "            LanguageCode='en'\n",
    "        )\n",
    "        print(f\"   ‚úì Sentiment: {sentiment_response['Sentiment']}\")\n",
    "        \n",
    "        # Entities\n",
    "        entities_response = comprehend.detect_entities(\n",
    "            Text=text_to_analyze,\n",
    "            LanguageCode='en'\n",
    "        )\n",
    "        print(f\"   ‚úì Found {len(entities_response['Entities'])} entities\")\n",
    "        \n",
    "        # Key phrases\n",
    "        phrases_response = comprehend.detect_key_phrases(\n",
    "            Text=text_to_analyze,\n",
    "            LanguageCode='en'\n",
    "        )\n",
    "        print(f\"   ‚úì Found {len(phrases_response['KeyPhrases'])} key phrases\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error analyzing text: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Compile results\n",
    "    print(\"\\nüìä Step 3: Compiling results...\")\n",
    "    \n",
    "    analysis_result = {\n",
    "        'document': document_key,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'extracted_text': extracted_text,\n",
    "        'sentiment': {\n",
    "            'overall': sentiment_response['Sentiment'],\n",
    "            'scores': sentiment_response['SentimentScore']\n",
    "        },\n",
    "        'entities': [\n",
    "            {\n",
    "                'text': e['Text'],\n",
    "                'type': e['Type'],\n",
    "                'score': e['Score']\n",
    "            }\n",
    "            for e in entities_response['Entities']\n",
    "        ],\n",
    "        'key_phrases': [\n",
    "            {\n",
    "                'text': p['Text'],\n",
    "                'score': p['Score']\n",
    "            }\n",
    "            for p in phrases_response['KeyPhrases']\n",
    "        ],\n",
    "        'metadata': {\n",
    "            'text_length': len(extracted_text),\n",
    "            'word_count': len(extracted_text.split())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 4: Save results\n",
    "    print(\"\\nüíæ Step 4: Saving results to S3...\")\n",
    "    \n",
    "    try:\n",
    "        # Save extracted text\n",
    "        text_key = document_key.replace('uploads/', 'processed/text/')\n",
    "        s3.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=text_key,\n",
    "            Body=extracted_text.encode('utf-8')\n",
    "        )\n",
    "        print(f\"   ‚úì Text: {text_key}\")\n",
    "        \n",
    "        # Save analysis JSON\n",
    "        analysis_key = document_key.replace('uploads/', 'processed/analysis/')\n",
    "        # Remove extension and add _analysis.json\n",
    "        analysis_key = analysis_key.rsplit('.', 1)[0] + '_analysis.json'\n",
    "        s3.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=analysis_key,\n",
    "            Body=json.dumps(analysis_result, indent=2).encode('utf-8'),\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "        print(f\"   ‚úì Analysis: {analysis_key}\")\n",
    "        \n",
    "        # Create and save summary report\n",
    "        summary = create_summary_report(analysis_result)\n",
    "        summary_key = analysis_key.replace('.json', '_summary.txt')\n",
    "        s3.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=summary_key,\n",
    "            Body=summary.encode('utf-8')\n",
    "        )\n",
    "        print(f\"   ‚úì Summary: {summary_key}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error saving results: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Pipeline complete!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return analysis_result\n",
    "\n",
    "\n",
    "def create_summary_report(analysis_result):\n",
    "    \"\"\"Generate human-readable summary\"\"\"\n",
    "    report = f\"\"\"\n",
    "Document Analysis Report\n",
    "Generated: {analysis_result['timestamp']}\n",
    "==========================================================\n",
    "\n",
    "Document: {analysis_result['document']}\n",
    "Word Count: {analysis_result['metadata']['word_count']}\n",
    "Characters: {analysis_result['metadata']['text_length']}\n",
    "\n",
    "SENTIMENT ANALYSIS\n",
    "------------------\n",
    "Overall Sentiment: {analysis_result['sentiment']['overall']}\n",
    "Confidence Scores:\n",
    "  Positive: {analysis_result['sentiment']['scores']['Positive']:.1%}\n",
    "  Negative: {analysis_result['sentiment']['scores']['Negative']:.1%}\n",
    "  Neutral:  {analysis_result['sentiment']['scores']['Neutral']:.1%}\n",
    "  Mixed:    {analysis_result['sentiment']['scores']['Mixed']:.1%}\n",
    "\n",
    "ENTITIES DETECTED\n",
    "-----------------\n",
    "\"\"\"\n",
    "    \n",
    "    # Group entities by type\n",
    "    entity_types = {}\n",
    "    for entity in analysis_result['entities']:\n",
    "        etype = entity['type']\n",
    "        if etype not in entity_types:\n",
    "            entity_types[etype] = []\n",
    "        entity_types[etype].append(entity['text'])\n",
    "    \n",
    "    if entity_types:\n",
    "        for etype, items in entity_types.items():\n",
    "            report += f\"\\n{etype}:\\n\"\n",
    "            for item in set(items):  # Remove duplicates\n",
    "                report += f\"  - {item}\\n\"\n",
    "    else:\n",
    "        report += \"\\nNo entities detected.\\n\"\n",
    "    \n",
    "    report += \"\\nKEY PHRASES\\n-----------\\n\"\n",
    "    if analysis_result['key_phrases']:\n",
    "        for phrase in analysis_result['key_phrases'][:10]:\n",
    "            report += f\"  - {phrase['text']}\\n\"\n",
    "    else:\n",
    "        report += \"No key phrases detected.\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "# Test the pipeline on your document\n",
    "print(\"Testing complete pipeline...\\n\")\n",
    "\n",
    "result = process_document_pipeline(\n",
    "    bucket=bucket_name,\n",
    "    document_key=document_key\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"\\nüìã Quick Summary:\")\n",
    "    print(f\"  Sentiment: {result['sentiment']['overall']}\")\n",
    "    print(f\"  Entities: {len(result['entities'])}\")\n",
    "    print(f\"  Key Phrases: {len(result['key_phrases'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269e21b-23d0-4d34-85c9-cd3dc5b8b8cc",
   "metadata": {},
   "source": [
    "**Adding a second document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e77da74-1c23-4ddd-8f70-97145628d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Second document uploaded\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting pipeline for: document-analysis/uploads/sample_feedback.txt\n",
      "============================================================\n",
      "\n",
      "üìÑ Step 1: Extracting text...\n",
      "   ‚úì Extracted 428 characters from text file\n",
      "\n",
      "üîç Step 2: Analyzing with Comprehend...\n",
      "   ‚úì Sentiment: POSITIVE\n",
      "   ‚úì Found 7 entities\n",
      "   ‚úì Found 17 key phrases\n",
      "\n",
      "üìä Step 3: Compiling results...\n",
      "\n",
      "üíæ Step 4: Saving results to S3...\n",
      "   ‚úì Text: document-analysis/processed/text/sample_feedback.txt\n",
      "   ‚úì Analysis: document-analysis/processed/analysis/sample_feedback_analysis.json\n",
      "   ‚úì Summary: document-analysis/processed/analysis/sample_feedback_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline complete!\n",
      "============================================================\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting pipeline for: document-analysis/uploads/negative_feedback.txt\n",
      "============================================================\n",
      "\n",
      "üìÑ Step 1: Extracting text...\n",
      "   ‚úì Extracted 384 characters from text file\n",
      "\n",
      "üîç Step 2: Analyzing with Comprehend...\n",
      "   ‚úì Sentiment: NEGATIVE\n",
      "   ‚úì Found 5 entities\n",
      "   ‚úì Found 12 key phrases\n",
      "\n",
      "üìä Step 3: Compiling results...\n",
      "\n",
      "üíæ Step 4: Saving results to S3...\n",
      "   ‚úì Text: document-analysis/processed/text/negative_feedback.txt\n",
      "   ‚úì Analysis: document-analysis/processed/analysis/negative_feedback_analysis.json\n",
      "   ‚úì Summary: document-analysis/processed/analysis/negative_feedback_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline complete!\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "AGGREGATE ANALYSIS\n",
      "======================================================================\n",
      "Total documents processed: 2\n",
      "\n",
      "Sentiment Distribution:\n",
      "  Positive: 1\n",
      "  Negative: 1\n",
      "  Neutral:  0\n",
      "\n",
      "Average entities per document: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Create a second sample document (negative feedback)\n",
    "negative_feedback = \"\"\"\n",
    "Customer Complaint\n",
    "\n",
    "Name: Jane Doe\n",
    "Date: November 6, 2024\n",
    "Product: Widget Pro 2000\n",
    "\n",
    "Feedback:\n",
    "I'm very disappointed with this product. The quality is poor and it broke \n",
    "after just two days of use. Customer service was unhelpful and took forever \n",
    "to respond. I would not recommend this to anyone. Complete waste of money.\n",
    "\n",
    "Would you recommend to others? No\n",
    "Overall satisfaction: 2/10\n",
    "\"\"\"\n",
    "\n",
    "with open('negative_feedback.txt', 'w') as f:\n",
    "    f.write(negative_feedback)\n",
    "\n",
    "# Upload to S3\n",
    "s3.upload_file(\n",
    "    'negative_feedback.txt',\n",
    "    bucket_name,\n",
    "    'document-analysis/uploads/negative_feedback.txt'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Second document uploaded\")\n",
    "\n",
    "# Process both documents\n",
    "documents = [\n",
    "    'document-analysis/uploads/sample_feedback.txt',\n",
    "    'document-analysis/uploads/negative_feedback.txt'\n",
    "]\n",
    "\n",
    "results = []\n",
    "for doc in documents:\n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    result = process_document_pipeline(bucket_name, doc)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "    time.sleep(1)  # Brief pause between documents\n",
    "\n",
    "# Aggregate analysis\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"AGGREGATE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total documents processed: {len(results)}\")\n",
    "\n",
    "positive_count = sum(1 for r in results if r['sentiment']['overall'] == 'POSITIVE')\n",
    "negative_count = sum(1 for r in results if r['sentiment']['overall'] == 'NEGATIVE')\n",
    "neutral_count = sum(1 for r in results if r['sentiment']['overall'] == 'NEUTRAL')\n",
    "\n",
    "print(f\"\\nSentiment Distribution:\")\n",
    "print(f\"  Positive: {positive_count}\")\n",
    "print(f\"  Negative: {negative_count}\")\n",
    "print(f\"  Neutral:  {neutral_count}\")\n",
    "\n",
    "avg_entities = sum(len(r['entities']) for r in results) / len(results)\n",
    "print(f\"\\nAverage entities per document: {avg_entities:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb037c6-e33e-4fa9-856f-60692d5d247a",
   "metadata": {},
   "source": [
    "**More Sample Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26f6f8be-311d-4150-99b9-bff955652318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and uploading sample documents...\n",
      "\n",
      "‚úÖ Uploaded: negative_feedback.txt\n",
      "‚úÖ Uploaded: neutral_review.txt\n",
      "‚úÖ Uploaded: support_ticket.txt\n",
      "‚úÖ Uploaded: employee_feedback.txt\n",
      "‚úÖ Uploaded: customer_email.txt\n",
      "\n",
      "‚úÖ Created and uploaded 5 new documents\n",
      "\n",
      "All documents in S3:\n",
      "  ‚Ä¢ customer_email.txt (0.2 KB)\n",
      "  ‚Ä¢ employee_feedback.txt (0.9 KB)\n",
      "  ‚Ä¢ negative_feedback.txt (0.7 KB)\n",
      "  ‚Ä¢ neutral_review.txt (0.7 KB)\n",
      "  ‚Ä¢ sample_feedback.txt (0.4 KB)\n",
      "  ‚Ä¢ support_ticket.txt (0.7 KB)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Sample Document 2: Negative Feedback\n",
    "negative_feedback = \"\"\"\n",
    "Customer Complaint Form\n",
    "\n",
    "Name: Jane Doe\n",
    "Date: November 6, 2024\n",
    "Product: Widget Pro 2000\n",
    "Order ID: WP-98765\n",
    "\n",
    "Complaint:\n",
    "I'm extremely disappointed with this product. The quality is terrible and it \n",
    "broke after just two days of normal use. I contacted customer service three \n",
    "times and received no helpful response. The wait time was over an hour each call.\n",
    "\n",
    "This is completely unacceptable for a product at this price point. I demand a \n",
    "full refund immediately. I will never purchase from this company again and will \n",
    "be sharing my experience on social media.\n",
    "\n",
    "Would you recommend to others? Absolutely not\n",
    "Overall satisfaction: 1/10\n",
    "Status: URGENT - Awaiting refund\n",
    "\"\"\"\n",
    "\n",
    "# Sample Document 3: Neutral/Mixed Review\n",
    "neutral_review = \"\"\"\n",
    "Product Review\n",
    "\n",
    "Reviewer: Mike Johnson\n",
    "Date: November 5, 2024\n",
    "Product: Widget Pro 2000\n",
    "Rating: 3/5 stars\n",
    "\n",
    "Review:\n",
    "The Widget Pro 2000 is an okay product with both strengths and weaknesses. \n",
    "\n",
    "Pros:\n",
    "- Easy to set up and use\n",
    "- Reasonable price point\n",
    "- Nice design and build quality\n",
    "\n",
    "Cons:\n",
    "- Battery life could be better\n",
    "- Limited features compared to competitors\n",
    "- Instructions were unclear in some sections\n",
    "\n",
    "Overall, it's a decent product but nothing exceptional. It gets the job done \n",
    "for basic use cases. I might consider other options if I were buying again, \n",
    "but it's not bad for the price.\n",
    "\n",
    "Would I recommend? Maybe, depends on your needs\n",
    "Overall: Average product, meets expectations but doesn't exceed them\n",
    "\"\"\"\n",
    "\n",
    "# Sample Document 4: Technical Support Ticket\n",
    "support_ticket = \"\"\"\n",
    "Technical Support Ticket\n",
    "\n",
    "Ticket ID: SUP-45123\n",
    "Customer: Sarah Williams\n",
    "Product: Widget Pro 2000\n",
    "Date Opened: November 4, 2024\n",
    "Priority: High\n",
    "\n",
    "Issue Description:\n",
    "Device fails to connect to WiFi network after latest firmware update (v2.3.1).\n",
    "Error message displays: \"Connection timeout - unable to reach server\"\n",
    "\n",
    "Steps already taken:\n",
    "1. Restarted device multiple times\n",
    "2. Reset network settings\n",
    "3. Tried different WiFi networks\n",
    "4. Uninstalled and reinstalled app\n",
    "\n",
    "System Information:\n",
    "- Firmware: v2.3.1\n",
    "- App Version: 4.2.0\n",
    "- Device Model: WP2000-X\n",
    "- Location: Seattle, Washington\n",
    "\n",
    "Customer is frustrated as device is needed for work. Requesting urgent \n",
    "technical assistance or replacement unit.\n",
    "\n",
    "Status: Open - Awaiting Technical Team Response\n",
    "\"\"\"\n",
    "\n",
    "# Sample Document 5: Positive Review from Employee\n",
    "employee_feedback = \"\"\"\n",
    "Internal Product Feedback - Q4 2024\n",
    "\n",
    "Employee: Robert Chen\n",
    "Department: Sales\n",
    "Region: West Coast\n",
    "Date: November 1, 2024\n",
    "\n",
    "Product Assessment: Widget Pro 2000\n",
    "\n",
    "I've been demonstrating the Widget Pro 2000 to clients for the past quarter \n",
    "and the response has been overwhelmingly positive. Customers love the intuitive \n",
    "interface and the reliability of the device.\n",
    "\n",
    "Key Strengths:\n",
    "- Easy to demonstrate features\n",
    "- Strong value proposition\n",
    "- Excellent customer satisfaction post-purchase\n",
    "- Low return rate (under 2%)\n",
    "\n",
    "The product has helped me close 15 deals this quarter, representing $250,000 \n",
    "in revenue. The Amazon integration and Microsoft Office compatibility are \n",
    "particularly impressive selling points.\n",
    "\n",
    "Recommendation: Continue investment in this product line. Consider expanding \n",
    "the lineup with additional models for enterprise customers.\n",
    "\n",
    "Overall Assessment: Excellent product with strong market potential\n",
    "Confidence Level: Very High\n",
    "\"\"\"\n",
    "\n",
    "# Sample Document 6: Short Customer Email\n",
    "customer_email = \"\"\"\n",
    "Subject: Quick Question\n",
    "\n",
    "Hi there,\n",
    "\n",
    "I received my Widget Pro 2000 yesterday. Setup was super easy! \n",
    "\n",
    "One question - does it work with Google Home? The manual doesn't mention it.\n",
    "\n",
    "Thanks!\n",
    "Lisa Martinez\n",
    "New York, NY\n",
    "\"\"\"\n",
    "\n",
    "# Dictionary of all documents\n",
    "documents = {\n",
    "    'negative_feedback.txt': negative_feedback,\n",
    "    'neutral_review.txt': neutral_review,\n",
    "    'support_ticket.txt': support_ticket,\n",
    "    'employee_feedback.txt': employee_feedback,\n",
    "    'customer_email.txt': customer_email\n",
    "}\n",
    "\n",
    "# Save and upload all documents\n",
    "print(\"Creating and uploading sample documents...\\n\")\n",
    "\n",
    "for filename, content in documents.items():\n",
    "    # Save locally\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_key = f'document-analysis/uploads/{filename}'\n",
    "    s3.upload_file(\n",
    "        filename,\n",
    "        bucket_name,\n",
    "        s3_key\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Uploaded: {filename}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created and uploaded {len(documents)} new documents\")\n",
    "print(\"\\nAll documents in S3:\")\n",
    "\n",
    "# List all documents in uploads folder\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix='document-analysis/uploads/'\n",
    ")\n",
    "\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        filename = obj['Key'].split('/')[-1]\n",
    "        size_kb = obj['Size'] / 1024\n",
    "        print(f\"  ‚Ä¢ {filename} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a00d46-94c5-4562-ae31-a2abb0092655",
   "metadata": {},
   "source": [
    "**Process Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb0e4650-17b0-4f54-a8e1-9961c98d67bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 documents to process\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üìÑ Processing Document 1/6\n",
      "======================================================================\n",
      "üöÄ Starting pipeline for: document-analysis/uploads/customer_email.txt\n",
      "============================================================\n",
      "\n",
      "üìÑ Step 1: Extracting text...\n",
      "   ‚úì Extracted 215 characters from text file\n",
      "\n",
      "üîç Step 2: Analyzing with Comprehend...\n",
      "   ‚úì Sentiment: POSITIVE\n",
      "   ‚úì Found 6 entities\n",
      "   ‚úì Found 9 key phrases\n",
      "\n",
      "üìä Step 3: Compiling results...\n",
      "\n",
      "üíæ Step 4: Saving results to S3...\n",
      "   ‚úì Text: document-analysis/processed/text/customer_email.txt\n",
      "   ‚úì Analysis: document-analysis/processed/analysis/customer_email_analysis.json\n",
      "   ‚úì Summary: document-analysis/processed/analysis/customer_email_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline complete!\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìÑ Processing Document 2/6\n",
      "======================================================================\n",
      "üöÄ Starting pipeline for: document-analysis/uploads/employee_feedback.txt\n",
      "============================================================\n",
      "\n",
      "üìÑ Step 1: Extracting text...\n",
      "   ‚úì Extracted 941 characters from text file\n",
      "\n",
      "üîç Step 2: Analyzing with Comprehend...\n",
      "   ‚úì Sentiment: POSITIVE\n",
      "   ‚úì Found 14 entities\n",
      "   ‚úì Found 39 key phrases\n",
      "\n",
      "üìä Step 3: Compiling results...\n",
      "\n",
      "üíæ Step 4: Saving results to S3...\n",
      "   ‚úì Text: document-analysis/processed/text/employee_feedback.txt\n",
      "   ‚úì Analysis: document-analysis/processed/analysis/employee_feedback_analysis.json\n",
      "   ‚úì Summary: document-analysis/processed/analysis/employee_feedback_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline complete!\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìÑ Processing Document 3/6\n",
      "======================================================================\n",
      "üöÄ Starting pipeline for: document-analysis/uploads/negative_feedback.txt\n",
      "============================================================\n",
      "\n",
      "üìÑ Step 1: Extracting text...\n",
      "   ‚úì Extracted 667 characters from text file\n",
      "\n",
      "üîç Step 2: Analyzing with Comprehend...\n",
      "   ‚úì Sentiment: NEGATIVE\n",
      "   ‚úì Found 9 entities\n",
      "   ‚úì Found 24 key phrases\n",
      "\n",
      "üìä Step 3: Compiling results...\n",
      "\n",
      "üíæ Step 4: Saving results to S3...\n",
      "   ‚úì Text: document-analysis/processed/text/negative_feedback.txt\n",
      "   ‚úì Analysis: document-analysis/processed/analysis/negative_feedback_analysis.json\n",
      "   ‚úì Summary: document-analysis/processed/analysis/negative_feedback_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline complete!\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìÑ Processing Document 4/6\n",
      "======================================================================\n",
      "üöÄ Starting pipeline for: document-analysis/uploads/neutral_review.txt\n",
      "============================================================\n",
      "\n",
      "üìÑ Step 1: Extracting text...\n",
      "   ‚úì Extracted 712 characters from text file\n",
      "\n",
      "üîç Step 2: Analyzing with Comprehend...\n",
      "   ‚úì Sentiment: MIXED\n",
      "   ‚úì Found 6 entities\n",
      "   ‚úì Found 27 key phrases\n",
      "\n",
      "üìä Step 3: Compiling results...\n",
      "\n",
      "üíæ Step 4: Saving results to S3...\n",
      "   ‚úì Text: document-analysis/processed/text/neutral_review.txt\n",
      "   ‚úì Analysis: document-analysis/processed/analysis/neutral_review_analysis.json\n",
      "   ‚úì Summary: document-analysis/processed/analysis/neutral_review_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline complete!\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìÑ Processing Document 5/6\n",
      "======================================================================\n",
      "üöÄ Starting pipeline for: document-analysis/uploads/sample_feedback.txt\n",
      "============================================================\n",
      "\n",
      "üìÑ Step 1: Extracting text...\n",
      "   ‚úì Extracted 428 characters from text file\n",
      "\n",
      "üîç Step 2: Analyzing with Comprehend...\n",
      "   ‚úì Sentiment: POSITIVE\n",
      "   ‚úì Found 7 entities\n",
      "   ‚úì Found 17 key phrases\n",
      "\n",
      "üìä Step 3: Compiling results...\n",
      "\n",
      "üíæ Step 4: Saving results to S3...\n",
      "   ‚úì Text: document-analysis/processed/text/sample_feedback.txt\n",
      "   ‚úì Analysis: document-analysis/processed/analysis/sample_feedback_analysis.json\n",
      "   ‚úì Summary: document-analysis/processed/analysis/sample_feedback_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline complete!\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìÑ Processing Document 6/6\n",
      "======================================================================\n",
      "üöÄ Starting pipeline for: document-analysis/uploads/support_ticket.txt\n",
      "============================================================\n",
      "\n",
      "üìÑ Step 1: Extracting text...\n",
      "   ‚úì Extracted 745 characters from text file\n",
      "\n",
      "üîç Step 2: Analyzing with Comprehend...\n",
      "   ‚úì Sentiment: NEUTRAL\n",
      "   ‚úì Found 9 entities\n",
      "   ‚úì Found 36 key phrases\n",
      "\n",
      "üìä Step 3: Compiling results...\n",
      "\n",
      "üíæ Step 4: Saving results to S3...\n",
      "   ‚úì Text: document-analysis/processed/text/support_ticket.txt\n",
      "   ‚úì Analysis: document-analysis/processed/analysis/support_ticket_analysis.json\n",
      "   ‚úì Summary: document-analysis/processed/analysis/support_ticket_analysis_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úÖ Pipeline complete!\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üéâ BATCH PROCESSING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Successfully processed 6/6 documents\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Get all documents in uploads folder\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix='document-analysis/uploads/'\n",
    ")\n",
    "\n",
    "if 'Contents' not in response:\n",
    "    print(\"No documents found!\")\n",
    "else:\n",
    "    document_keys = [obj['Key'] for obj in response['Contents'] if not obj['Key'].endswith('/')]\n",
    "    \n",
    "    print(f\"Found {len(document_keys)} documents to process\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    for i, doc_key in enumerate(document_keys, 1):\n",
    "        print(f\"\\n\\nüìÑ Processing Document {i}/{len(document_keys)}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            result = process_document_pipeline(bucket_name, doc_key)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "            time.sleep(1)  # Brief pause between documents\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {doc_key}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"üéâ BATCH PROCESSING COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n‚úÖ Successfully processed {len(results)}/{len(document_keys)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301dbad-299d-4a39-810a-58a5e78672fe",
   "metadata": {},
   "source": [
    "**Aggregate Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34880432-9800-4de0-87a8-826af20ac4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================================\n",
      "üìä AGGREGATE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üé≠ Sentiment Distribution:\n",
      "  MIXED: 1 documents (16.7%)\n",
      "  NEGATIVE: 1 documents (16.7%)\n",
      "  NEUTRAL: 1 documents (16.7%)\n",
      "  POSITIVE: 3 documents (50.0%)\n",
      "\n",
      "üè∑Ô∏è Most Common Entity Types:\n",
      "  QUANTITY: 14 occurrences\n",
      "  DATE: 9 occurrences\n",
      "  COMMERCIAL_ITEM: 6 occurrences\n",
      "  PERSON: 6 occurrences\n",
      "  TITLE: 6 occurrences\n",
      "\n",
      "üìà Document Statistics:\n",
      "  Total documents: 6\n",
      "  Total words: 564\n",
      "  Average words per document: 94\n",
      "  Average entities per document: 8.5\n",
      "  Average key phrases per document: 25.3\n",
      "\n",
      "üîç Notable Documents:\n",
      "  Most positive: sample_feedback.txt\n",
      "    Positive confidence: 100.0%\n",
      "  Most negative: negative_feedback.txt\n",
      "    Negative confidence: 100.0%\n",
      "\n",
      "üíæ Aggregate report saved: s3://sagemaker-us-east-2-854757836160/document-analysis/processed/analysis/aggregate_report.txt\n",
      "\n",
      "\n",
      "AGGREGATE DOCUMENT ANALYSIS REPORT\n",
      "Generated: 2025-11-08 19:21:52\n",
      "==========================================================\n",
      "\n",
      "SUMMARY\n",
      "-------\n",
      "Total Documents Processed: 6\n",
      "Total Words: 564\n",
      "Average Words per Document: 94\n",
      "\n",
      "SENTIMENT DISTRIBUTION\n",
      "----------------------\n",
      "MIXED: 1 documents (16.7%)\n",
      "NEGATIVE: 1 documents (16.7%)\n",
      "NEUTRAL: 1 documents (16.7%)\n",
      "POSITIVE: 3 documents (50.0%)\n",
      "\n",
      "\n",
      "ENTITY ANALYSIS\n",
      "---------------\n",
      "Total Entities Detected: 51\n",
      "Average per Document: 8.5\n",
      "\n",
      "Most Common Entity Types:\n",
      "  QUANTITY: 14\n",
      "  DATE: 9\n",
      "  COMMERCIAL_ITEM: 6\n",
      "  PERSON: 6\n",
      "  TITLE: 6\n",
      "\n",
      "\n",
      "KEY INSIGHTS\n",
      "------------\n",
      "- Most positive document: sample_feedback.txt (100.0% confidence)\n",
      "- Most negative document: negative_feedback.txt (100.0% confidence)\n",
      "- Average key phrases per document: 25.3\n",
      "\n",
      "DOCUMENTS PROCESSED\n",
      "-------------------\n",
      "- customer_email.txt: POSITIVE\n",
      "- employee_feedback.txt: POSITIVE\n",
      "- negative_feedback.txt: NEGATIVE\n",
      "- neutral_review.txt: MIXED\n",
      "- sample_feedback.txt: POSITIVE\n",
      "- support_ticket.txt: NEUTRAL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"üìä AGGREGATE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts = {}\n",
    "for r in results:\n",
    "    sentiment = r['sentiment']['overall']\n",
    "    sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n",
    "\n",
    "print(\"\\nüé≠ Sentiment Distribution:\")\n",
    "for sentiment, count in sorted(sentiment_counts.items()):\n",
    "    percentage = (count / len(results)) * 100\n",
    "    print(f\"  {sentiment}: {count} documents ({percentage:.1f}%)\")\n",
    "\n",
    "# Entity analysis\n",
    "all_entity_types = {}\n",
    "for r in results:\n",
    "    for entity in r['entities']:\n",
    "        etype = entity['type']\n",
    "        all_entity_types[etype] = all_entity_types.get(etype, 0) + 1\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Most Common Entity Types:\")\n",
    "sorted_entities = sorted(all_entity_types.items(), key=lambda x: x[1], reverse=True)\n",
    "for etype, count in sorted_entities[:5]:\n",
    "    print(f\"  {etype}: {count} occurrences\")\n",
    "\n",
    "# Document statistics\n",
    "total_words = sum(r['metadata']['word_count'] for r in results)\n",
    "avg_words = total_words / len(results)\n",
    "total_entities = sum(len(r['entities']) for r in results)\n",
    "avg_entities = total_entities / len(results)\n",
    "total_phrases = sum(len(r['key_phrases']) for r in results)\n",
    "avg_phrases = total_phrases / len(results)\n",
    "\n",
    "print(\"\\nüìà Document Statistics:\")\n",
    "print(f\"  Total documents: {len(results)}\")\n",
    "print(f\"  Total words: {total_words:,}\")\n",
    "print(f\"  Average words per document: {avg_words:.0f}\")\n",
    "print(f\"  Average entities per document: {avg_entities:.1f}\")\n",
    "print(f\"  Average key phrases per document: {avg_phrases:.1f}\")\n",
    "\n",
    "# Identify extremes\n",
    "most_positive = max(results, key=lambda x: x['sentiment']['scores']['Positive'])\n",
    "most_negative = max(results, key=lambda x: x['sentiment']['scores']['Negative'])\n",
    "\n",
    "print(\"\\nüîç Notable Documents:\")\n",
    "print(f\"  Most positive: {most_positive['document'].split('/')[-1]}\")\n",
    "print(f\"    Positive confidence: {most_positive['sentiment']['scores']['Positive']:.1%}\")\n",
    "print(f\"  Most negative: {most_negative['document'].split('/')[-1]}\")\n",
    "print(f\"    Negative confidence: {most_negative['sentiment']['scores']['Negative']:.1%}\")\n",
    "\n",
    "# Save aggregate report\n",
    "aggregate_report = f\"\"\"\n",
    "AGGREGATE DOCUMENT ANALYSIS REPORT\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "==========================================================\n",
    "\n",
    "SUMMARY\n",
    "-------\n",
    "Total Documents Processed: {len(results)}\n",
    "Total Words: {total_words:,}\n",
    "Average Words per Document: {avg_words:.0f}\n",
    "\n",
    "SENTIMENT DISTRIBUTION\n",
    "----------------------\n",
    "\"\"\"\n",
    "\n",
    "for sentiment, count in sorted(sentiment_counts.items()):\n",
    "    percentage = (count / len(results)) * 100\n",
    "    aggregate_report += f\"{sentiment}: {count} documents ({percentage:.1f}%)\\n\"\n",
    "\n",
    "aggregate_report += f\"\"\"\n",
    "\n",
    "ENTITY ANALYSIS\n",
    "---------------\n",
    "Total Entities Detected: {total_entities}\n",
    "Average per Document: {avg_entities:.1f}\n",
    "\n",
    "Most Common Entity Types:\n",
    "\"\"\"\n",
    "\n",
    "for etype, count in sorted_entities[:5]:\n",
    "    aggregate_report += f\"  {etype}: {count}\\n\"\n",
    "\n",
    "aggregate_report += f\"\"\"\n",
    "\n",
    "KEY INSIGHTS\n",
    "------------\n",
    "- Most positive document: {most_positive['document'].split('/')[-1]} ({most_positive['sentiment']['scores']['Positive']:.1%} confidence)\n",
    "- Most negative document: {most_negative['document'].split('/')[-1]} ({most_negative['sentiment']['scores']['Negative']:.1%} confidence)\n",
    "- Average key phrases per document: {avg_phrases:.1f}\n",
    "\n",
    "DOCUMENTS PROCESSED\n",
    "-------------------\n",
    "\"\"\"\n",
    "\n",
    "for r in results:\n",
    "    filename = r['document'].split('/')[-1]\n",
    "    sentiment = r['sentiment']['overall']\n",
    "    aggregate_report += f\"- {filename}: {sentiment}\\n\"\n",
    "\n",
    "# Save aggregate report to S3\n",
    "aggregate_key = 'document-analysis/processed/analysis/aggregate_report.txt'\n",
    "s3.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=aggregate_key,\n",
    "    Body=aggregate_report.encode('utf-8')\n",
    ")\n",
    "\n",
    "print(f\"\\nüíæ Aggregate report saved: s3://{bucket_name}/{aggregate_key}\")\n",
    "print(\"\\n\" + aggregate_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c533f7a-3bed-4f41-92ab-797b6613235f",
   "metadata": {},
   "source": [
    "**Verify Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36a5055b-f44d-4cd6-a077-2d35b4beaad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================================\n",
      "üìÅ FINAL FILE STRUCTURE\n",
      "======================================================================\n",
      "\n",
      "üì§ Uploads:\n",
      "  ‚Ä¢ customer_email.txt\n",
      "  ‚Ä¢ employee_feedback.txt\n",
      "  ‚Ä¢ negative_feedback.txt\n",
      "  ‚Ä¢ neutral_review.txt\n",
      "  ‚Ä¢ sample_feedback.txt\n",
      "  ‚Ä¢ support_ticket.txt\n",
      "\n",
      "üìÑ Processed Text:\n",
      "  ‚Ä¢ customer_email.txt\n",
      "  ‚Ä¢ employee_feedback.txt\n",
      "  ‚Ä¢ negative_feedback.txt\n",
      "  ‚Ä¢ neutral_review.txt\n",
      "  ‚Ä¢ sample_feedback.txt\n",
      "  ‚Ä¢ support_ticket.txt\n",
      "\n",
      "üìä Analysis Results:\n",
      "  ‚Ä¢ aggregate_report.txt\n",
      "  ‚Ä¢ customer_email_analysis.json\n",
      "  ‚Ä¢ customer_email_analysis_summary.txt\n",
      "  ‚Ä¢ employee_feedback_analysis.json\n",
      "  ‚Ä¢ employee_feedback_analysis_summary.txt\n",
      "  ‚Ä¢ negative_feedback_analysis.json\n",
      "  ‚Ä¢ negative_feedback_analysis_summary.txt\n",
      "  ‚Ä¢ neutral_review_analysis.json\n",
      "  ‚Ä¢ neutral_review_analysis_summary.txt\n",
      "  ‚Ä¢ sample_feedback_analysis.json\n",
      "  ‚Ä¢ sample_feedback_analysis_summary.txt\n",
      "  ‚Ä¢ support_ticket_analysis.json\n",
      "  ‚Ä¢ support_ticket_analysis_summary.txt\n",
      "\n",
      "‚úÖ Project structure complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"üìÅ FINAL FILE STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check uploads\n",
    "print(\"\\nüì§ Uploads:\")\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix='document-analysis/uploads/')\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        if not obj['Key'].endswith('/'):\n",
    "            print(f\"  ‚Ä¢ {obj['Key'].split('/')[-1]}\")\n",
    "\n",
    "# Check processed text\n",
    "print(\"\\nüìÑ Processed Text:\")\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix='document-analysis/processed/text/')\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        if not obj['Key'].endswith('/'):\n",
    "            print(f\"  ‚Ä¢ {obj['Key'].split('/')[-1]}\")\n",
    "\n",
    "# Check analysis results\n",
    "print(\"\\nüìä Analysis Results:\")\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix='document-analysis/processed/analysis/')\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        if not obj['Key'].endswith('/'):\n",
    "            print(f\"  ‚Ä¢ {obj['Key'].split('/')[-1]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Project structure complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a25d6-2e89-46ed-b3a3-6022cf887849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
