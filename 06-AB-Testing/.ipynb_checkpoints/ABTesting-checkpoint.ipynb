{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0d79e9-09cc-4be9-9058-4e5fb6808321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Quick Titanic data prep (if you don't have it already)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe2a8c3-0c01-4695-ba50-f5bd128d22e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training data uploaded to: s3://sagemaker-us-east-2-854757836160/titanic-data/train/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_880/2217492332.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
      "/tmp/ipykernel_880/2217492332.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Download Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Basic preprocessing\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "\n",
    "# Prepare for XGBoost (label first, then features)\n",
    "df_xgb = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "\n",
    "# Split and save\n",
    "train_data, test_data = train_test_split(df_xgb, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "train_path = f's3://{bucket}/titanic-data/train/train.csv'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('titanic-data/train/train.csv').upload_file('train.csv')\n",
    "\n",
    "print(f\"âœ… Training data uploaded to: {train_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29211f8f-f673-49e7-8319-8146d514ef73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-11-17-19-57-50-238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bucket: sagemaker-us-east-2-854757836160\n",
      "Using role: arn:aws:iam::854757836160:role/service-role/AmazonSageMaker-ExecutionRole-20251026T175451\n",
      "\n",
      "============================================================\n",
      "TRAINING MODEL A (Conservative)\n",
      "============================================================\n",
      "2025-11-17 19:57:52 Starting - Starting the training job...\n",
      "2025-11-17 19:58:25 Downloading - Downloading input data...\n",
      "2025-11-17 19:58:50 Downloading - Downloading the training image......\n",
      "2025-11-17 19:59:51 Training - Training image download completed. Training in progress.\n",
      "2025-11-17 19:59:51 Uploading - Uploading generated training model.\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-11-17 19:59:47.040 ip-10-0-154-248.us-east-2.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-11-17 19:59:47.062 ip-10-0-154-248.us-east-2.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] Train matrix has 712 rows and 7 columns\u001b[0m\n",
      "\u001b[34m[2025-11-17 19:59:47.439 ip-10-0-154-248.us-east-2.compute.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-11-17 19:59:47.440 ip-10-0-154-248.us-east-2.compute.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-11-17 19:59:47.440 ip-10-0-154-248.us-east-2.compute.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-11-17 19:59:47.440 ip-10-0-154-248.us-east-2.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-11-17:19:59:47:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[19:59:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.56609\u001b[0m\n",
      "\u001b[34m[2025-11-17 19:59:47.452 ip-10-0-154-248.us-east-2.compute.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-11-17 19:59:47.456 ip-10-0-154-248.us-east-2.compute.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.49682\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.45271\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.42627\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.41011\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.39937\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.38988\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.38310\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.37640\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.37339\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.36840\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.36628\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.36294\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.35911\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.35708\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.35364\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.35256\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.34795\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.34653\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.34566\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.34297\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.34132\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.34009\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.33719\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.33166\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.33062\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.32593\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.32516\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.32337\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.32098\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.31720\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.31612\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.31342\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.31188\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.31084\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.30949\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.30815\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.30464\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.30404\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.30109\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.29933\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.29673\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.29579\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.29512\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.29480\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.29387\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.29311\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.29256\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.29176\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.28887\u001b[0m\n",
      "\n",
      "2025-11-17 20:00:04 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-11-17-20-00-37-185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 99\n",
      "Billable seconds: 99\n",
      "\n",
      "âœ… Model A trained successfully!\n",
      "Model A artifact: s3://sagemaker-us-east-2-854757836160/ab-test/model-a/sagemaker-xgboost-2025-11-17-19-57-50-238/output/model.tar.gz\n",
      "\n",
      "============================================================\n",
      "TRAINING MODEL B (Aggressive - Better Accuracy)\n",
      "============================================================\n",
      "2025-11-17 20:00:37 Starting - Starting the training job...\n",
      "2025-11-17 20:01:02 Starting - Preparing the instances for training...\n",
      "2025-11-17 20:01:19 Downloading - Downloading input data...\n",
      "2025-11-17 20:01:45 Downloading - Downloading the training image...\n",
      "2025-11-17 20:02:31 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-11-17 20:02:37.562 ip-10-0-78-198.us-east-2.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-11-17 20:02:37.585 ip-10-0-78-198.us-east-2.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] Train matrix has 712 rows and 7 columns\u001b[0m\n",
      "\u001b[34m[2025-11-17 20:02:37.962 ip-10-0-78-198.us-east-2.compute.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-11-17 20:02:37.962 ip-10-0-78-198.us-east-2.compute.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-11-17 20:02:37.963 ip-10-0-78-198.us-east-2.compute.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-11-17 20:02:37.963 ip-10-0-78-198.us-east-2.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-11-17:20:02:37:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[20:02:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.64263\u001b[0m\n",
      "\u001b[34m[2025-11-17 20:02:37.975 ip-10-0-78-198.us-east-2.compute.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-11-17 20:02:37.978 ip-10-0-78-198.us-east-2.compute.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.59896\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.56384\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.53235\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.50801\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.48507\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.46583\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.44857\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.43278\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.41903\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.40817\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.39859\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.38816\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.38063\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.37257\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.36530\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.35876\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.35268\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.34768\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.34206\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.33724\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.33223\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.32800\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.32348\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.31973\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.31691\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.31389\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.31002\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.30731\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.30313\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.29995\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.29753\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.29360\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.28995\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.28813\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.28507\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.28358\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.28153\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.27911\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.27592\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.27309\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.27124\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.26896\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.26792\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.26593\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.26446\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.26273\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.26057\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.25962\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.25876\u001b[0m\n",
      "\u001b[34m[50]#011train-logloss:0.25709\u001b[0m\n",
      "\u001b[34m[51]#011train-logloss:0.25471\u001b[0m\n",
      "\u001b[34m[52]#011train-logloss:0.25260\u001b[0m\n",
      "\u001b[34m[53]#011train-logloss:0.25119\u001b[0m\n",
      "\u001b[34m[54]#011train-logloss:0.24965\u001b[0m\n",
      "\u001b[34m[55]#011train-logloss:0.24809\u001b[0m\n",
      "\u001b[34m[56]#011train-logloss:0.24689\u001b[0m\n",
      "\u001b[34m[57]#011train-logloss:0.24403\u001b[0m\n",
      "\u001b[34m[58]#011train-logloss:0.24278\u001b[0m\n",
      "\u001b[34m[59]#011train-logloss:0.24080\u001b[0m\n",
      "\u001b[34m[60]#011train-logloss:0.23907\u001b[0m\n",
      "\u001b[34m[61]#011train-logloss:0.23759\u001b[0m\n",
      "\u001b[34m[62]#011train-logloss:0.23635\u001b[0m\n",
      "\u001b[34m[63]#011train-logloss:0.23452\u001b[0m\n",
      "\u001b[34m[64]#011train-logloss:0.23330\u001b[0m\n",
      "\u001b[34m[65]#011train-logloss:0.23130\u001b[0m\n",
      "\u001b[34m[66]#011train-logloss:0.22967\u001b[0m\n",
      "\u001b[34m[67]#011train-logloss:0.22871\u001b[0m\n",
      "\u001b[34m[68]#011train-logloss:0.22730\u001b[0m\n",
      "\u001b[34m[69]#011train-logloss:0.22576\u001b[0m\n",
      "\u001b[34m[70]#011train-logloss:0.22468\u001b[0m\n",
      "\u001b[34m[71]#011train-logloss:0.22348\u001b[0m\n",
      "\u001b[34m[72]#011train-logloss:0.22305\u001b[0m\n",
      "\u001b[34m[73]#011train-logloss:0.22185\u001b[0m\n",
      "\u001b[34m[74]#011train-logloss:0.22062\u001b[0m\n",
      "\u001b[34m[75]#011train-logloss:0.21922\u001b[0m\n",
      "\u001b[34m[76]#011train-logloss:0.21816\u001b[0m\n",
      "\u001b[34m[77]#011train-logloss:0.21664\u001b[0m\n",
      "\u001b[34m[78]#011train-logloss:0.21577\u001b[0m\n",
      "\u001b[34m[79]#011train-logloss:0.21461\u001b[0m\n",
      "\u001b[34m[80]#011train-logloss:0.21385\u001b[0m\n",
      "\u001b[34m[81]#011train-logloss:0.21247\u001b[0m\n",
      "\u001b[34m[82]#011train-logloss:0.21159\u001b[0m\n",
      "\u001b[34m[83]#011train-logloss:0.21079\u001b[0m\n",
      "\u001b[34m[84]#011train-logloss:0.20996\u001b[0m\n",
      "\u001b[34m[85]#011train-logloss:0.20870\u001b[0m\n",
      "\u001b[34m[86]#011train-logloss:0.20748\u001b[0m\n",
      "\u001b[34m[87]#011train-logloss:0.20655\u001b[0m\n",
      "\u001b[34m[88]#011train-logloss:0.20554\u001b[0m\n",
      "\u001b[34m[89]#011train-logloss:0.20464\u001b[0m\n",
      "\u001b[34m[90]#011train-logloss:0.20391\u001b[0m\n",
      "\u001b[34m[91]#011train-logloss:0.20300\u001b[0m\n",
      "\u001b[34m[92]#011train-logloss:0.20185\u001b[0m\n",
      "\u001b[34m[93]#011train-logloss:0.20141\u001b[0m\n",
      "\u001b[34m[94]#011train-logloss:0.20035\u001b[0m\n",
      "\u001b[34m[95]#011train-logloss:0.19945\u001b[0m\n",
      "\u001b[34m[96]#011train-logloss:0.19823\u001b[0m\n",
      "\u001b[34m[97]#011train-logloss:0.19736\u001b[0m\n",
      "\u001b[34m[98]#011train-logloss:0.19656\u001b[0m\n",
      "\u001b[34m[99]#011train-logloss:0.19625\u001b[0m\n",
      "\n",
      "2025-11-17 20:02:54 Uploading - Uploading generated training model\n",
      "2025-11-17 20:02:54 Completed - Training job completed\n",
      "Training seconds: 94\n",
      "Billable seconds: 94\n",
      "\n",
      "âœ… Model B trained successfully!\n",
      "Model B artifact: s3://sagemaker-us-east-2-854757836160/ab-test/model-b/sagemaker-xgboost-2025-11-17-20-00-37-185/output/model.tar.gz\n",
      "\n",
      "============================================================\n",
      "PHASE 1 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Both models trained and saved to S3:\n",
      "  Model A: s3://sagemaker-us-east-2-854757836160/ab-test/model-a/sagemaker-xgboost-2025-11-17-19-57-50-238/output/model.tar.gz\n",
      "  Model B: s3://sagemaker-us-east-2-854757836160/ab-test/model-b/sagemaker-xgboost-2025-11-17-20-00-37-185/output/model.tar.gz\n",
      "\n",
      "Ready for Phase 2: Deployment!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using bucket: {bucket}\")\n",
    "print(f\"Using role: {role}\")\n",
    "\n",
    "# Get XGBoost container\n",
    "container = image_uris.retrieve('xgboost', sess.boto_region_name, '1.5-1')\n",
    "\n",
    "# Path to your training data (assuming you have it from Week 2)\n",
    "train_path = f's3://{bucket}/titanic-data/train/'\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MODEL A (Conservative)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Model A: Conservative hyperparameters\n",
    "xgb_model_a = Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{bucket}/ab-test/model-a/',\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Conservative hyperparameters (faster, simpler)\n",
    "xgb_model_a.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    num_round=50,         # Fewer rounds\n",
    "    max_depth=3,          # Shallower trees\n",
    "    eta=0.3,              # Higher learning rate\n",
    "    subsample=1.0         # Use all data\n",
    ")\n",
    "\n",
    "# Train Model A\n",
    "xgb_model_a.fit({'train': TrainingInput(train_path, content_type='text/csv')})\n",
    "\n",
    "print(f\"\\nâœ… Model A trained successfully!\")\n",
    "print(f\"Model A artifact: {xgb_model_a.model_data}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MODEL B (Aggressive - Better Accuracy)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Model B: Aggressive hyperparameters\n",
    "xgb_model_b = Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{bucket}/ab-test/model-b/',\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Aggressive hyperparameters (more accurate, slower)\n",
    "xgb_model_b.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,        # More rounds\n",
    "    max_depth=6,          # Deeper trees\n",
    "    eta=0.1,              # Lower learning rate\n",
    "    subsample=0.8         # Use 80% of data per tree\n",
    ")\n",
    "\n",
    "# Train Model B\n",
    "xgb_model_b.fit({'train': TrainingInput(train_path, content_type='text/csv')})\n",
    "\n",
    "print(f\"\\nâœ… Model B trained successfully!\")\n",
    "print(f\"Model B artifact: {xgb_model_b.model_data}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nBoth models trained and saved to S3:\")\n",
    "print(f\"  Model A: {xgb_model_a.model_data}\")\n",
    "print(f\"  Model B: {xgb_model_b.model_data}\")\n",
    "print(\"\\nReady for Phase 2: Deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45224549-36d8-431f-8be8-eaae614627e9",
   "metadata": {},
   "source": [
    "**Multi-Variant Endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbce6671-bf8e-462b-a73f-701c329d2bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 2: DEPLOY MULTI-VARIANT ENDPOINT\n",
      "============================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "Checking for existing endpoints to clean up...\n",
      "------------------------------------------------------------\n",
      "Found existing endpoint: titanic-ab-test-20251117-200652\n",
      "  Deleting to free up resources...\n",
      "  âœ… Deleted\n",
      "\n",
      "Waiting 30 seconds for resources to be released...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Creating Model A and Model B\n",
      "------------------------------------------------------------\n",
      "Creating Model A: model-a-20251117201745\n",
      "âœ… Model A created successfully\n",
      "Creating Model B: model-b-20251117201746\n",
      "âœ… Model B created successfully\n",
      "\n",
      "------------------------------------------------------------\n",
      "Deploying both models with 80/20 traffic split\n",
      "Using ml.t2.medium instances (within quota)\n",
      "------------------------------------------------------------\n",
      "Creating endpoint config: ab-config-20251117201747\n",
      "âœ… Endpoint config created\n",
      "\n",
      "Creating endpoint: titanic-ab-test-20251117-201747\n",
      "(This takes ~8-10 minutes...)\n",
      "Waiting for endpoint to be in service...\n",
      "\n",
      "âœ… Endpoint deployed successfully!\n",
      "\n",
      "============================================================\n",
      "PHASE 2 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Endpoint: titanic-ab-test-20251117-201747\n",
      "Traffic split:\n",
      "  VariantA (Conservative): 80%\n",
      "  VariantB (Aggressive):   20%\n",
      "\n",
      "Both variants are now live and receiving traffic!\n",
      "\n",
      "ðŸ’¾ Save this endpoint name: titanic-ab-test-20251117-201747\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: DEPLOY MULTI-VARIANT ENDPOINT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get SageMaker client\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "# ============================================================\n",
    "# First, check for and delete any existing endpoints\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Checking for existing endpoints to clean up...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "try:\n",
    "    response = client.list_endpoints(\n",
    "        StatusEquals='InService',\n",
    "        MaxResults=100\n",
    "    )\n",
    "    \n",
    "    for endpoint in response['Endpoints']:\n",
    "        if 'titanic-ab-test' in endpoint['EndpointName']:\n",
    "            print(f\"Found existing endpoint: {endpoint['EndpointName']}\")\n",
    "            print(f\"  Deleting to free up resources...\")\n",
    "            client.delete_endpoint(EndpointName=endpoint['EndpointName'])\n",
    "            print(f\"  âœ… Deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "print(\"\\nWaiting 30 seconds for resources to be released...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# ============================================================\n",
    "# Create and Register Both Models\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Creating Model A and Model B\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Create Model A\n",
    "model_a_name = f'model-a-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "print(f\"Creating Model A: {model_a_name}\")\n",
    "\n",
    "client.create_model(\n",
    "    ModelName=model_a_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': xgb_model_a.model_data\n",
    "    },\n",
    "    ExecutionRoleArn=role\n",
    ")\n",
    "print(\"âœ… Model A created successfully\")\n",
    "\n",
    "# Create Model B\n",
    "model_b_name = f'model-b-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "print(f\"Creating Model B: {model_b_name}\")\n",
    "\n",
    "client.create_model(\n",
    "    ModelName=model_b_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': xgb_model_b.model_data\n",
    "    },\n",
    "    ExecutionRoleArn=role\n",
    ")\n",
    "print(\"âœ… Model B created successfully\")\n",
    "\n",
    "# ============================================================\n",
    "# Deploy Both Models with Traffic Split (using ml.t2.medium)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Deploying both models with 80/20 traffic split\")\n",
    "print(\"Using ml.t2.medium instances (within quota)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Create endpoint configuration\n",
    "endpoint_config_name = f'ab-config-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "endpoint_name = f'titanic-ab-test-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"Creating endpoint config: {endpoint_config_name}\")\n",
    "\n",
    "client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'VariantA',\n",
    "            'ModelName': model_a_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.t2.medium',  # Smaller instance\n",
    "            'InitialVariantWeight': 80\n",
    "        },\n",
    "        {\n",
    "            'VariantName': 'VariantB',\n",
    "            'ModelName': model_b_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.t2.medium',  # Smaller instance\n",
    "            'InitialVariantWeight': 20\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"âœ… Endpoint config created\")\n",
    "\n",
    "# Create endpoint\n",
    "print(f\"\\nCreating endpoint: {endpoint_name}\")\n",
    "print(\"(This takes ~8-10 minutes...)\")\n",
    "\n",
    "client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "# Wait for endpoint to be in service\n",
    "print(\"Waiting for endpoint to be in service...\")\n",
    "waiter = client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "print(f\"\\nâœ… Endpoint deployed successfully!\")\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nEndpoint: {endpoint_name}\")\n",
    "print(f\"Traffic split:\")\n",
    "print(f\"  VariantA (Conservative): 80%\")\n",
    "print(f\"  VariantB (Aggressive):   20%\")\n",
    "print(f\"\\nBoth variants are now live and receiving traffic!\")\n",
    "\n",
    "# Create predictor for Phase 3\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Save this endpoint name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba77e1-2239-4aad-9819-8b23b116989c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
